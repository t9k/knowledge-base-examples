# 智能法律问答应用中的一些技术问题

## 目的

本文旨在讨论智能法律问答应用在开发过程中的一些技术问题，以指导进一步的设计及实现。

## 业务需求

智能法律问答应用的核心业务需求包括：

1. **快速在线响应法律咨询**：系统需快速响应用户法律咨询请求，自动生成专业、合规的答复，覆盖特定的法律专业领域。  
2. **基于法律和案件回答**：系统需检索并引用法律法规、司法解释、内部制度和历史案例，确保答复具备法律依据与实践支撑。  
3. **自动标注法律依据**：每条应答必须附带可溯源的条文或制度出处，满足法律部对规范性和可追责性的要求。  
4. **支持上下文连续的多轮问答**：面向复杂咨询场景，系统需识别用户意图变化，维持对话上下文，提升交互连续性与用户体验。  
5. **生成结构化风控报告**：对高频问题进行聚类分析与风险建模，定期输出咨询热点及潜在风险点，辅助法务团队做出策略调整。  
6. **满足企业级性能标准**：系统问答准确率达到 90% 以上，首次响应时间（TTFT）小于 3 秒，并支持数千人次的并发访问，保障大规模使用场景下的稳定性与可用性。

围绕上述需求，我们从两个方向进行技术方案讨论和设计：1）知识库的构建，2） Agent 模块的设计。

## 知识库（Knowledge Base）构建

在设计一个面向法律领域的知识库时，其核心挑战远不止于存储海量文本，更关键的是必须预先设计一个在**检索效果**、**数据时效性、系统性能**和**服务可靠性**等维度上均表现卓越的完整体系。单纯的文本存储与单一的检索方式，无法满足法律场景的复杂需求。因此在构建之初，我们即引入向量数据库的技术路径，并选用高性能、高可扩展的 Milvus 作为底层引擎。与此同时，为实现法律场景下真正高质量的检索与问答体验，我们还必须前瞻性地解决一系列从应用到架构的问题：

1. **语义鸿沟**：用户的提问方式通常是口语化的、模糊的，而法律文本的表达则是高度专业、严谨的。如何跨越两者之间的语义鸿沟，准确理解用户的真实意图，是一大难题。  
2. **关键词的刚性需求**：与模糊的意图理解相对，法律检索中又包含大量必须精确匹配的关键词，如特定的法律术语、案件当事人名称、案件中出现的专有名词等。  
3. **上下文的完整性**：法律文书篇幅长、逻辑链条严密。检索出的信息片段如果缺乏必要的上下文，不仅可能导致信息失真，也会严重影响大语言模型生成回答的准确性和全面性。  
4. **结构化信息的筛选能力**：海量的法律数据中，用户往往需要根据明确的非文本属性（如特定法院、案件类型、审理年份）进行筛选。系统必须具备高效、灵活的元数据过滤能力，以快速缩小检索范围。  
5. **检索结果的改进**：向量检索等初步筛选方法能快速召回大量相关文档，但结果中常混杂着次要或不完全匹配的内容。在初筛结果的基础上，通过更精细的二次排序（重排）提炼出最精准的少数文档，是提升最终检索和回答质量的关键方法。  
6. **索引的选择**：高效的检索依赖于底层的索引结构。不同的索引技术在查询速度、召回率、内存消耗上各有优劣。必须根据应用场景进行技术选型，以在性能和成本之间取得最佳平衡。  
7. **知识库的更新**：法律知识动态变化，但全量更新耗时巨大且易中断服务。因此，如何实现无缝、平滑的版本升级，在保障数据时效性的同时维持服务连续性，是一项工程挑战。  
8. **生产级架构的扩展与高可用**：一个真正的生产级应用，必须能应对未来数据和访问量的增长，并能在组件故障时保持稳定。因此，架构设计必须具备弹性伸缩和高可用特性，以保障系统的长期可靠性。

为全面应对上述挑战，我们采用融合多种检索策略与系统架构优化的综合性方案。从语义理解到精确重排，从索引优化到弹性部署，系统在检索效果、性能、时效性与可靠性等各方面，均以生产级标准为目标，构建高质量的法律知识库。

### 数据导入与知识库版本管理

#### 数据导入的挑战：增量更新 vs. 全量更新

法律知识库的生命力在于其时效性。法律体系处于持续变化中：新法不断颁布填补空白，旧法会被废止失效，现行法律条文也经常修订。这些立、改、废的动态若未及时、准确地反映在知识库中，用户依据过时或错误信息进行决策、签订合同或处理纠纷，将面临巨大的法律风险。

在维护一个动态的法律知识库时，数据更新策略至关重要。常见的更新方式有两种：增量更新和全量更新，但它们各自面临着不同的挑战。

1. **增量更新（Incremental Update）**:  
   * **过程**：指在现有知识库基础上，直接添加、修改或删除部分数据。  
   * **问题**：这种方式虽然看起来快速、资源消耗小，但长期运行会导致一系列严重问题。首先是**检索效果下降**，因为像 HNSW 这样的 ANN 索引在增量添加数据后，其图结构可能变得不均衡，导致查询性能和召回率降低。其次，复杂的更新逻辑容易引发**数据不一致**，例如，若更新流程中某个步骤失败，可能导致新旧数据共存、索引与原始数据不同步等混乱状态。  
2. **全量更新（Full Update）**:  
   * **过程**：指彻底重建整个知识库。这包括读取所有原始法律数据，重新进行分段、向量化，并构建全新的检索引擎索引。  
   * **优势**：全量更新可以从根本上解决数据不一致和索引性能衰退的问题，确保每一次发布的知识库都处于最佳的、最一致的状态。  
   * **挑战**：其最大的缺点是**服务中断**。重建一个大规模的知识库通常需要数小时甚至更长时间，在此期间，如果直接替换线上服务，将导致法律检索功能长时间不可用，这对于要求高可用性的应用是不可接受的。

增量更新所带来的问题比较棘手，不容易解决，因而我们采用全量更新。为应对全量更新的服务中断问题，我们必须采用一种能够实现平滑过渡的版本发布策略。

#### 使用 Argo 流水线实现蓝绿发布

为解决全量更新导致的停机问题，我们引入了**蓝绿发布（Blue-Green Deployment）**策略，并利用 **Argo Workflows** 这一云原生工作流引擎来自动化整个发布过程。

蓝绿发布的核心思想是通过维护两套完全相同但相互独立的生产环境（“蓝色”环境和“绿色”环境）来实现零停机更新。

* **蓝色环境 (Blue)**：代表当前正在线上提供服务的、稳定运行的知识库版本（例如 v1）。  
* **绿色环境 (Green)**：代表一个全新的、离线构建的知识库版本（例如 v2）。

**Argo 工作流驱动的发布流程如下：**

1. **触发构建**：当有新的数据或模型更新时，Argo 工作流被触发，开始在“绿色”环境中构建一个全新的知识库。  
2. **离线全量构建**：流水线会自动执行所有建库步骤：拉取最新的全量法律数据、分段、调用 LLM 提取元数据、生成密集与稀疏向量，并为它们在 Milvus 中构建优化的 HNSW 和倒排索引。整个过程完全在离线环境中进行，不会对线上“蓝色”环境造成任何干扰。  
3. **验证与测试**：在“绿色”环境构建完成后，Argo 流水线会自动运行一系列预设的测试用例，以验证新知识库的数据完整性、检索准确率和查询性能是否符合上线标准。  
4. **流量切换**：一旦“绿色”环境验证通过，只需一步简单的操作（如修改网关配置），即可将所有用户流量从“蓝色”环境瞬间切换到“绿色”环境。此时，“绿色”环境正式成为新的线上服务。这个切换过程极快，用户几乎无感知，从而实现了**零停机时间**。  
5. **下线旧版**：在确认新的“蓝色”环境稳定运行一段时间后，原先的旧版环境（现在是空闲的“绿色”环境）可以被安全下线或作为下一次更新的备用环境，从而释放资源。

通过将全量更新与 Argo 流水线驱动的蓝绿发布相结合，我们既能确保法律知识库的数据一致性和最优检索性能，又能保障业务的高可用性，实现安全、可靠、无中断的知识库版本迭代。

### 多种检索方法

参考：

1. [https://zilliz.com/learn/sparse-and-dense-embeddings](https://zilliz.com/learn/sparse-and-dense-embeddings)  
2. [https://zilliz.com/learn/hybrid-search-combining-text-and-image](https://zilliz.com/learn/hybrid-search-combining-text-and-image)  
3. [https://milvus.io/docs/filtered-search.md](https://milvus.io/docs/filtered-search.md)

法律文本的复杂性和多样性，要求检索引擎必须具备多维度的信息捕捉能力。向量数据库提供了多种先进的检索方法，每种方法都有其独特的优势和适用场景。为了实现全面、精准且高效的法律信息定位，有必要支持这些不同的技术。

#### 密集检索（Dense Retrieval）/ 语义检索（Semantic Retrieval）

密集检索利用深度学习模型将文本转化为高维度的向量（Dense Embeddings），这些向量能够捕捉文本的深层语义信息，而不仅仅是字面上的关键词。

1. **定义**： 密集检索是一种基于“语义相似性”的检索方法。它通过深度学习模型（如 BERT）将查询和文档都转换成数值向量，然后在向量空间中寻找与查询向量最接近的文档向量。这种方法的核心在于理解查询的“意图”，而不是简单匹配词语。  
2. **适合解决什么问题**： 此方法非常适合处理那些使用自然语言、口语化描述，或者不包含精确法律术语的查询。当用户无法准确描述一个法律问题，但能大致描述其情境和意图时，密集检索能够超越关键词的限制，找到语义上相关的法律条文或案例。  
3. **举例说明**：  
   * **例一（模糊的自然语言提问）**：用户查询：“**公司拖欠工资我怎么办？**”  
   * **检索结果**：系统能够理解“公司”即“用人单位”，“拖欠工资”在法律上对应“未及时足额支付劳动报酬”。因此，它能成功检索到《劳动合同法》中相关的条款、司法解释以及处理“劳动争议”的案例，即使用户的查询中并未出现任何专业术语。  
   * **例二（自然语言描述案情）**：用户查询：“**趁司机不在，偷窃货车上的加油卡，后转卖**”。  
   * **检索结果**：系统会提取这段描述的语义核心——“盗窃财物”和“转卖获利”，从而关联到“盗窃罪”的构成要件、量刑标准以及相关的案例，即使查询没有直接使用“盗窃罪”这个关键词。

#### 稀疏检索（Sparse Retrieval）/ 全文检索（Full-Text Retrieval）

稀疏检索，通常以传统的全文检索技术（如 BM25）为代表，它通过构建关键词索引来实现检索，核心是匹配查询和文档中的共同词语。

1. **定义**： 稀疏检索是一种基于“关键词匹配”的精确检索方法。它将文本表示为一种稀疏向量（Sparse Embeddings），向量的维度对应于词汇表的大小，向量中的值则代表了特定词语在文档中的重要性（如 TF-IDF 或 BM25 分数）。这种方法对专有名词、术语、代码等具有极高的识别和匹配能力。  
2. **适合解决什么问题**： 当用户的查询包含明确的、不可替代的关键词时，稀疏检索表现最佳。这包括专有名词（人名、公司名、地名）、特定的法律术语、案件编号、法条代码等。它能确保查询中的这些关键信息被精确无误地找到。  
3. **举例说明**：  
   * **例一（专有名词）**：用户在检索案件时，输入一个特定的化学品名称“**溴氰菊酯**”。  
   * **检索结果**：系统会返回文本中出现“溴氰菊酯”或类似关键词（如“氰戊菊酯”）的文档，例如涉及该化学品的刑事案件或产品责任纠纷，确保了检索结果的高度相关性。  
   * **例二（地名）**：用户输入一个具体的地址“**湖塘镇花园新村**”。  
   * **检索结果**：系统会返回文本中出现“湖塘镇花园新村”这一地点或包含类似关键词的文档，实现精准定位。

#### 混合检索（Hybrid Search）

混合检索是一种将密集检索的“语义理解”能力与稀疏检索的“关键词精确性”相结合的先进检索策略，旨在集两者之所长，提供更全面、更精准的检索结果。

1. **定义**： 混合检索是一种融合策略，它同时执行密集检索和稀疏检索，然后通过特定的算法（如 Reciprocal Rank Fusion, RRF）将两者的结果智能地合并和重排序。这样，检索结果既考虑了语义上的相关性，也兼顾了关键词的精确匹配。  
2. **适合解决什么问题**： 混合检索是通用性最强的检索方法，适用于绝大多数法律检索场景。无论是包含专业术语的复杂自然语言查询，还是模糊的案情描述，混合检索都能提供鲁棒性极强的检索表现，有效避免单一检索方法的短板。  
3. **举例说明**：  
   * **用户查询**：“**哪些罪犯适用死刑，哪些不适用？**”  
   * **检索分析**：  
     * **稀疏检索**会精确匹配关键词“**死刑**”，确保所有返回的文档都与这个核心刑罚直接相关。  
     * **密集检索**则会理解问题的深层语义，即“**适用**”与“**不适用**”的条件。它会去寻找那些讨论“死刑的适用条件”、“法定从重情节”、“法定从轻或免除情节”、“限制适用死刑的情形”（如针对孕妇、未成年人）等内容的法律条文和权威解读。  
   * **检索结果**：混合检索将两方面结果结合，最终会优先呈现那些既包含“死刑”关键词，又在内容上深入阐述了其适用与限制条件的《刑法》条文。这比单独使用任何一种方法都更加全面。

#### 元数据检索/过滤（Metadata Filtering）

元数据是描述数据的数据，在法律文本中，它通常指案件的结构化信息，如法院、审判日期、案件类型、当事人等。元数据过滤允许在执行其他检索之前，对数据进行精确的筛选。

1. **定义**： 元数据检索/过滤是一种基于数据“结构化属性”的精确筛选方法。它允许用户根据文本之外的元数据字段（如日期、地点、类别等）设定严格的过滤条件，支持范围查询（如日期范围）、精确匹配（如法院名称）和逻辑组合（与、或、非）等复杂操作。  
2. **适合解决什么问题**： 当用户有明确的范围限制或需要对海量结果进行精确约束时，此方法至关重要。它能极大地缩小检索范围，提高检索效率和结果的准确性。元数据过滤可以与上述任意检索方法组合使用。  
3. **举例说明**：  
   * **用户需求**：用户需要精确查找特定的一类案件，其查询可以被解析为：“**检索辽宁省大连市中级人民法院2021年9月1日判决的民事案件**”。  
   * **执行过程**：  
     1. **元数据过滤**：系统将直接应用筛选条件，而不是进行文本内容的搜索。  
        * `court = "大连市中级人民法院"`  
        * `judgment_date = "2021-09-01"`  
     2. （可选）**组合检索**：可以在上述筛选出的结果范围内，再执行一个关键词或语义查询来进一步缩小范围，例如“房屋买卖合同纠纷”。  
   * **检索结果**：系统将只返回完全符合上述所有结构化属性的判决文书，实现了对海量法律文书的精准定位。

总而言之，法律检索需要结合密集检索（捕捉语义相似性）、稀疏检索（匹配关键词精确性）、混合检索（融合多策略优势）及元数据过滤（定位结构化属性），以兼顾法律文本的复杂语义、精确术语匹配、多维度关联分析及高效筛选需求，从而实现全面、精准且高效的法律信息定位。

### 重排 (Reranking)

参考：

1. [https://www.pinecone.io/learn/series/rag/rerankers/](https://www.pinecone.io/learn/series/rag/rerankers/)  
2. [https://zilliz.com/learn/optimize-rag-with-rerankers-the-role-and-tradeoffs](https://zilliz.com/learn/optimize-rag-with-rerankers-the-role-and-tradeoffs)

在 RAG 系统中，重排（Reranking）是提升检索质量与生成效果的关键策略。初始检索阶段通常使用向量搜索从大规模文档中快速找出 top k 个候选文档，但由于向量表示存在信息压缩和语义丢失，常常会遗漏关键内容。直接将大量文档传递给 LLM 以提升召回率，会因为上下文窗口限制和模型自身召回能力下降而适得其反。

因此，重排可以作为第二阶段，在初筛结果中进一步排序，仅保留最相关的少量文档传递给 LLM。重排模型通常是 cross-encoder，可以同时考虑查询和文档语义，实现更精细的匹配判断，虽然其计算成本更高。通过这种“两阶段检索 \+ 精准重排”的方式，系统在保持高检索精确率和召回率的同时，也提升了生成响应的准确性与上下文契合度。

在法律领域，重排的必要性还体现在：查询表达往往含糊或歧义，仅靠向量相似度易误召、漏召；法律文书篇幅长、结构复杂，准确理解查询与文书的深层匹配关系，必须依赖更强的语言理解能力。

### 索引（Index）类型选择

参考：

1. [https://milvus.io/docs/index-explained.md](https://milvus.io/docs/index-explained.md)  
2. [https://thedataquarry.com/blog/vector-db-3/](https://thedataquarry.com/blog/vector-db-3/)  
3. [https://zilliz.com/learn/bge-m3-and-splade-two-machine-learning-models-for-generating-sparse-embeddings](https://zilliz.com/learn/bge-m3-and-splade-two-machine-learning-models-for-generating-sparse-embeddings)

为实现密集检索、稀疏检索和混合检索，我们需要为每个 chunk 生成密集与稀疏两种向量，并为它们在 Milvus 向量数据库中配置适当的索引。

#### 密集向量 (Dense Vector) 与其索引

1. 向量生成：使用 Qwen3-Embedding-4B 模型生成密集向量。Qwen3 Embedding 系列模型在当下（2025 年 6 月）达到 SOTA 性能，8B 模型在 [MTEB leaderboard](https://huggingface.co/spaces/mteb/leaderboard) 排名第一，4B 模型在性能接近 8B 模型（平均分数 69.45 vs 70.58）的情况下参数只有它的一半，在性能、速度和资源消耗之间取得了比较好的平衡。需要注意的是，Qwen3 Embedding 系列模型不支持生成稀疏向量。  
2. 索引选择：对于存储在内存中的大规模密集向量，选择合适的近似最近邻（ANN）索引是平衡性能、成本和精度的关键。以下是几种业界主流索引的客观比较：

| 索引类型 | 核心原理 | 主要优势与考量 | 典型适用场景 |
| :---- | :---- | :---- | :---- |
| IVF | 通过聚类将向量分组，并建立倒排索引（Inverted File）。搜索时先筛选部分分组（候选集），再在其中进行精确搜索。 | 优势：构建速度快，内存占用显著低于图索引。 考量：性能和精度严重依赖 nprobe (查询的候选集数量) 参数的调节。 | 中等规模数据集（百万级），对资源消耗（内存）和索引构建速度有较高要求的场景。 |
| HNSW | 构建多层图结构（Hierarchical Navigable Small World）实现高效近邻搜索，利用分层导航和贪婪路由加速查找过程。 | 优势：精度高，查询速度快。 考量：构建成本和内存占用相对较高。 | 需要低延迟和高精度查询的高维数据集。 |
| SCANN | 结合向量量化（压缩）、聚类分桶和多阶段搜索等技术来加速搜索。 | 优势：查询速度和精度优秀，内存占用较低。 考量：对硬件性能有一定要求，支持多种优化策略。 | 需要高吞吐量的大规模数据集。 |
| DiskANN | 针对磁盘（SSD）存储优化的图索引算法，将大部分数据和索引结构保存在磁盘上。 | 优势：支持海量（十亿级以上）向量索引，内存占用极低。 考量：查询速度受限于磁盘 I/O，但仍远快于暴力搜索。 | 数据规模极大，无法全部加载到内存的场景。 |

选择考量：这几种索引覆盖了从资源敏感型到性能优先型的不同需求。

应用需求要点如下：

1. 优先保障低延迟和高召回  
2. 对吞吐量要求适中  
3. [top-k 较小](https://milvus.io/docs/index-explained.md)  
4. 需保留未来扩展数据规模的能力

HNSW 在延迟与召回之间实现了良好平衡，是当前主流高性能应用中的优选方案。对于 HNSW 的参数，我们选用一套高召回率的设置（M=24、efConstruction=400），代价是需要更多的内存和构建时间。

#### 稀疏向量 (Sparse Vector) 与倒排索引

1. 向量生成：使用 BGE-M3 模型生成稀疏向量。当下（2025 年 6 月）支持稀疏嵌入的开源模型并不多，BGE-M3 和 Splade 是这一领域最先进的模型，又考虑到 BGE-M3 支持多语言而 Splade 针对英文文本设计和训练，我们选择 BGE-M3 模型。  
2. 索引选择：唯一的选择就是 SPARSE\_INVERTED\_INDEX，它利用了倒排索引的原理，为稀疏数据创建了一种高效的搜索结构。

通过为不同特性的向量配置专用索引，我们能够最大化数据库的检索性能，为整个系统提供低延迟、高精度的检索能力，从而为上层的混合检索算法提供高质量的输入。

### 父子分段（Parent-Child Chunking）

参考：

1. [https://python.langchain.com/docs/how\_to/parent\_document\_retriever/](https://python.langchain.com/docs/how_to/parent_document_retriever/)  
2. [https://dify.ai/blog/introducing-parent-child-retrieval-for-enhanced-knowledge](https://dify.ai/blog/introducing-parent-child-retrieval-for-enhanced-knowledge)

父子分段（Parent-Child Chunking）是一种分层的文档切分与检索策略，常用于 RAG 系统。它将文档划分为两级结构：子段（child chunk）是更小的文本单元，用于提升查询的匹配精度，而父段（parent chunk）是包含子段的较大文本块，提供完整的上下文信息，供大模型生成更准确、丰富的回答。这种方法有效解决了检索中“精准 vs. 上下文”的矛盾，提升了响应质量和信息可用性。

对于裁判文书这类篇幅长、逻辑结构强的文档，设定几百长度（例如 256、512）的 chunk\_size，会导致案情描述、控辩双方观点、法院认定和判决结果等核心内容分散到多个 chunk，检索出的单个分块往往是上下文不完整的片段，不足以完整、全面地回答用户的问题。而设置更长的 chunk\_size，又会影响检索的准确性。

因此我们采用父子分段策略：向量检索基于更小粒度的子分块，以提升语义匹配的精准度；但在生成回答时，系统回溯并使用包含更加完整的上下文的父段落，确保大语言模型理解充分、回答准确。

### LLM 提取元数据

案件的裁判文书中包含大量的细节信息，例如日期、地点、人物、数额等，使用它们作为关键词检索时往往效果不佳（对于罕见的人名地名，采用稀疏向量检索效果尚可），可它们又确实是检索的重要参考信息。既然如此，我们可以利用 LLM 从 chunk 中提取这些关键信息作为额外的元数据，供过滤使用。

相较于依赖正则表达式（rule-based）的脆弱性和手动标注（manual）的高成本，LLM 凭借其强大的上下文理解和零样本/少样本学习能力，可以更鲁棒、更规模化地从非结构化文本中抽取出如 {"dates": "20191018", "locations": "沈阳市浑南区"} 这样的键值对。

指示 LLM 提取元数据的 prompt 见 [data/insert\_data\_cail2018.py](http://../data/insert_data_cail2018.py) 中的 `extract_metadata_with_llm` 函数。

### 可扩展性、高可用性与可靠性

对于一个生产级的法律知识库，其后台架构必须能支撑海量数据与高并发访问，并保障服务持续在线与数据安全。我们采用在 **Kubernetes** 上部署 **Milvus** 的云原生方案，来满足系统在**弹性扩展**、**高可用**、**数据可靠性**与**一致性**方面的严苛要求。

#### 弹性自动伸缩（Elastic Scalability）

为应对法律文书数据和用户查询量的动态变化，系统必须具备自动伸缩的能力。通过在 Kubernetes 上部署 Milvus，我们可以利用 [**Horizontal Pod Autoscaler（HPA）**](https://milvus.io/docs/hpa.md) ，实现对系统资源的智能调度。我们可以设定 CPU 或内存使用率等阈值，当用户查询高峰导致**查询节点（Query Node）** 负载升高时，HPA 会**自动增加**其副本数量以分摊压力；当高峰过后，又会自动缩减副本数，释放闲置资源。这种自动化的弹性伸缩机制，确保了系统在任何负载下都能维持最佳的服务性能与成本效益。

#### 数据扩容策略（Data Scaling Strategy）

随着业务发展，法律知识库收录的文本和向量数据规模会持续增长。处理这种可预见和超预期的增长，是架构设计中需要考虑的一环。

* **前瞻性的容量规划与资源配置**：在首次部署 Milvus 之前，进行容量规划是必要的一步。我们需要预估法律知识库在未来几年内可能达到的数据规模上限。基于这个预估值，可以利用 Milvus 官方提供的在线[配置工具（Sizing Tool）](https://milvus.io/tools/sizing)，精确计算出为满足该规模数据所需的各项资源，包括为 Milvus 各核心组件（如 Query Node, Index Node, Data Node）分配的 CPU、内存和磁盘规格。这种前瞻性的规划确保了初始部署的系统架构具备足够的冗余和扩展空间，能够平滑地容纳未来一段时间内数据的持续增长。  
* **平滑的在线扩容与无感迁移**：如果数据增长速度超出最初的预期，云原生架构的优势便体现出来。我们可以利用**蓝绿发布（Blue-Green Deployment）**的策略，实现对 Milvus 的无感扩容。具体操作为：在不影响现有“蓝色”生产环境的情况下，并行部署一个配置了更多资源的全新“绿色”Milvus 实例。待新实例的数据通过备份恢复或数据迁移工具同步完成后，只需将应用流量从旧的蓝色实例平滑切换至新的绿色实例。此过程实现了零停机时间的服务升级，确保了在应对超预期数据增长时，依然能够提供稳定、不间断的服务。

#### 高可用性（High Availability）

高可用性关注的是服务的持续在线，即最大化减少停机时间。我们通过 Milvus 的多副本机制与 Kubernetes 的自愈能力来实现。Milvus 的无状态组件（如查询节点）可以部署多个副本，Kubernetes 会自动进行负载均衡。一旦某个副本或其所在的服务器节点发生故障，Kubernetes 会立即将其流量切换到健康副本，并自动尝试恢复故障实例，整个过程对用户透明，从而有效避免单点故障，保障服务的连续性。

#### 数据可靠性与一致性（Data Reliability & Consistency）

数据的安全与一致是系统的生命线。Milvus 提供了多重机制来保障这一点：

* **事务日志确保写入一致性**：Milvus 内部通过维护**事务日志**来确保数据操作的原子性和持久性。任何数据的增、删、改操作都会先被记录到日志中，然后再应用到数据库。这种机制保证了即使在写入过程中发生故障，系统也能通过日志进行恢复，从而避免了数据损坏或状态不一致的问题。  
* **灵活的备份与恢复**：数据资产的安全性需要可靠的备份策略。Milvus 提供了相应的 API 和工具（如 MilvusDM），支持对知识库数据进行**全量备份**和**增量备份**。我们可以制定定期的备份计划，将数据快照存储在安全的对象存储中。一旦发生灾难性故障，可以利用这些备份快速恢复知识库到指定的时间点，最大限度地保障了数据的可靠性。

## Agent 设计

我们所构建的智能法律问答应用需要满足以下需求：

1. **深入理解用户意图**，即便用户的表述比较口语化或模糊。  
2. 能够以某种方式调用工具以灵活、精确、高效地**检索知识库**。  
3. **支持多轮对话**，有效管理对话历史，确保在持续交流中，回答能保持上下文的连贯性和准确性。  
4. ...

针对每一项需求，我们将比较多种潜在的技术方案，旨在为每个环节选择最合适的实现路径，以确保构建的 Agent 有更好的功能、表现和扩展性。

### 识别用户意图

参考：

1. [https://qwenlm.github.io/zh/blog/qwen3/](https://qwenlm.github.io/zh/blog/qwen3/)  
2. [https://docs.dify.ai/zh-hans/guides/workflow/node/question-classifier](https://docs.dify.ai/zh-hans/guides/workflow/node/question-classifier)

识别用户意图指的是 Agent 不仅要理解用户查询的字面含义，更要洞察其背后的真实需求和目标，以采取最恰当的行动（如检索法条、分析案情或解释概念）。精准的意图识别是调用正确工具、给出有效答复的前提。

法律场景下用户意图复杂多样，现归纳为以下几类典型场景，Agent 需要具备相应的处理能力：

1. **精确信息检索**：用户目标明确，需要查找特定的法律法规或案件。  
   * 期望的处理模式：Agent 应能准确提取查询中的实体（如法律名称、法条编号、案号），并调用知识库检索工具返回原文。  
   * 例如：“查找《民法典》关于离婚冷静期的规定” 或 “检索张三诉李四的案件判决书”。  
2. **开放式法律咨询**：用户描述一个具体情境或问题，寻求法律上的解释和指导。  
   * 期望的处理模式：Agent 需要理解口语化描述，将其解构成法律关键词，进行综合检索，并整合信息生成结构化的分析和建议。  
   * 例如：“房东不退押金怎么办？” 或 “‘诉讼时效’是什么意思？”。  
3. **基于上下文的追问**：用户在多轮对话中，针对已有的回答进行深入提问。  
   * 期望的处理模式：Agent 必须理解对话历史，将“这个案子”、“那条规定”等指代词与前文关联，进行聚焦分析或补充检索。  
   * 例如：（在得到法条后）“这条规定有例外情况吗？” 或 （在看到案件后）“这个案子的主要争议点是什么？”。  
4. **复杂情景分析**：用户提供一个包含多个法律要素的复杂情况，寻求综合性的责任评估或策略建议。  
   * 期望的处理模式：Agent 需要展现出规划和推理能力，将复杂问题分解为多个子任务，依次检索分析，最后给出全面的评估。  
   * 例如：“我开车撞了闯红灯的行人，但自己也超速了，责任怎么分？”。

在识别用户意图环节，考虑以下 2 种方案：

1. 启用 LLM 思考模式：Qwen3 系列模型支持思考模式（thinking mode）和非思考模式（non-thinking mode），并且在思考模式下支持设置 thinking budget。只需启用思考模式，LLM 就会自主在思考过程中识别用户意图。增加 thinking budget，LLM 对于用户意图的思考也会加深。  
2. 在 Workflow 中使用问题分类节点：在 Dify 等 LLM 应用开发平台的 Workflow 中，使用平台提供的问题分类节点，根据分类结果路由到不同的分支。

比较 2 种方案：

1. 方案 1 允许用户灵活设置思考的深度，而方案 2 的问题分类过程是固定的；  
2. 方案 2 需要为问题分类节点编写的 prompt 更加复杂；  
3. 方案 2 问题分类有哪些类型需要由人工定义，离散的类型也不够灵活；

因此我们选择方案 1。

### 检索知识库

参考：

1. [https://github.com/zilliztech/mcp-server-milvus](https://github.com/zilliztech/mcp-server-milvus)  
2. [https://docs.dify.ai/zh-hans/guides/workflow/node/knowledge-retrieval](https://docs.dify.ai/zh-hans/guides/workflow/node/knowledge-retrieval)

对于应用以何种方式调用工具以检索知识库，考虑以下 2 种方案：

1. 自主动态调用 MCP 工具：将多种检索方法（包含稀疏检索、密集检索、混合检索、元数据过滤、重排等）的实现封装成多个独立的 MCP 工具，通过部署 MCP Server 暴露，LLM 通过 MCP Client 调用。  
2. 在 Workflow 中使用知识检索节点：编写与 Dify 等 LLM 应用开发平台兼容的外部知识服务并部署，其中实现了多种检索方法；然后创建外部知识库连接外部知识服务，在 Workflow 中使用平台提供的知识检索节点。

比较 2 种方案：

1. 方案 1 在编写 MCP Server 时，我们可以灵活地增减工具，为工具配置适当的参数以实现精细控制，然后自主定义和实现完整的检索流程；而方案 2 的 Dify 外部知识服务只能实现一个固定的 API 端点，参数格式受限，无法扩展。例如，其请求体中无法指定使用的检索方法，也不支持添加新的工具或参数。  
2. 方案 1 部署的 MCP Server 与平台解耦，具备高度通用性。当前主流的 AI 应用开发平台（如 LangChain、Dify、n8n 等）均已支持 MCP Client，可灵活部署和迁移，亦可自主开发聊天客户端进行集成。而 Dify 的外部知识服务专属于 Dify 平台，一旦迁移到其他平台，则需重新开发，迁移成本高、通用性差。

因此考虑到可扩展性和可迁移性，我们选择方案 1。

### 管理多轮对话

参考：

1. [https://blog.csdn.net/Kiradzy/article/details/145063066](https://blog.csdn.net/Kiradzy/article/details/145063066)

对于应用在多轮对话中如何管理对话历史，以确保对话历史不超过模型的最大上下文窗口，同时尽量保持上下文的连贯性，我们考虑以下 3 种方案：

1. 保留最近 n 个 token 的对话历史：保留最近的若干轮对话，使得 token 总数刚好不超过 n，n 可以是模型的最大上下文窗口或为了保障推理性能的其他指定值。  
2. 保留最近 k 轮对话：保留最近的 k 轮对话，k 轮之前的对话会被丢弃；若最近 k 轮对话的 token 总数超过模型的最大上下文窗口，则保留最近的若干轮对话使得刚好不超过最大上下文窗口。  
3. 滚动式摘要：在方案 1 或方案 2 的基础上，当对话超出预设的 token 长度或轮次时，调用 LLM 将早期的对话内容进行总结，形成一段摘要，并将此摘要与近期对话合并作为新的上下文。

比较 3 种方案：

1. 方案 1 通过调整 n，既可适用短对话场景，又可适用长对话场景；方案 2 适用于短对话；方案 3 适用于需要保留一些背景信息或关键信息的长对话场景  
2. 方案 1 通过限制上下文长度保障了推理性能（例如多轮对话的 TTFT）  
3. 方案 1 能够灵活适配不同的对话场景（例如每轮对话的 token 数量较少时可以保留很多轮对话）  
4. 方案 3 调用 LLM 总结引入额外的计算开销和延迟

法律问答场景下，正确全面的回答往往需要完整的上下文、严谨的表述和丰富的细节信息，有损压缩过的总结对于回答的准确性可能有不利影响。方案 3 可能不完全适用于当前场景。再考虑到方案 1 的上述优势，我们这里选择方案 1。

## 总结和展望

我们构建了一个基于 RAG 与 MCP 框架的企业级智能法律问答应用，集成了法律法规和海量案例数据，结合多种检索方式与重排机制，并通过 LLM 提取元数据、采用父子分块策略，显著提升了检索效果和问答准确率。同时，借助 Qwen3-32B 模型驱动的智能 Agent，可动态调用工具、深入理解用户意图，满足企业对私有化部署和数据安全的高要求。

展望未来，该架构具备良好的通用性和可扩展性，有望推广至金融、医疗、科研等多个专业领域。随着数据规模不断扩大，并引入合同、规章等多样化文档，我们也将逐步构建法律知识图谱，以结构化方式表达复杂的法规和案例关联，从而进一步增强系统的理解力和可解释性，尽管这一过程在技术和资源上仍面临不小挑战。  
