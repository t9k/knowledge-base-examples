# 复盘智能法律问答应用

本文旨在回顾智能法律问答应用在开发过程中的关键技术选型，并对其优势进行分析讨论。

## 知识库构建

> 补充背景知识：[法律信息检索的核心挑战](https://chat.qwen.ai/s/3d82b5c1-a13e-44f2-ab88-e0845be5e375?fev=0.0.143)

### 多种检索方法

法律文本复杂多样，向量数据库的各种检索方法都有适用的场景，因此有必要都支持。这些检索方法包括：

1. 密集检索（语义检索）：能理解问题的真实语义，检索不受限于关键词。例如用户使用模糊的自然语言描述一个法律问题，没有使用精准的法律术语：“公司拖欠工资我怎么办？”；用户使用自然语言描述大致的案情，“趁司机不在，盗窃货车上的加油卡，后转卖”。

2. 稀疏检索（全文检索）：对专有名词、术语、代码等识别能力强，关键词匹配精确。例如用户使用专有名词、地名、人名、公司名等检索案件：“溴氰菊酯”、“湖塘镇花园新村”。

3. 混合检索：将密集检索与稀疏检索相结合，以发挥两者优势，既能理解自然语言的深层含义，又能精确匹配专有名词。混合检索适用于大多数场景。

4. 元数据检索/过滤：对数据进行精确的约束和过滤，支持范围、匹配、逻辑等多种复杂查询。例如用户根据元数据检索/筛选案件：“检索辽宁省大连市中级人民法院2021年9月1日判决的民事案件”。此方法也支持与其他检索方法（如混合检索）组合使用，以缩小和精确化检索范围。

总而言之，法律检索需要结合密集检索（捕捉语义相似性）、稀疏检索（匹配关键词精确性）、混合检索（融合多策略优势）及元数据过滤（定位结构化属性），以兼顾法律文本的复杂语义、精确术语匹配、多维度关联分析及高效筛选需求，从而实现全面、精准且高效的法律信息定位。

### 重排

在检索返回结果后，还可通过重排进一步优化排序，将最相关的内容呈现给 LLM。

重排的作用：
* 弥补检索召回不足：初始检索通常依赖向量距离、关键词匹配，可能漏掉语义更相关的文段。
* 强化上下文理解：重排模型可综合考虑查询意图与候选内容的上下文语义，提升相关性判断能力。
* 优化排序精度：通过对候选结果进行逐对比较或整体打分，显著提升 Top-K 精度。

重排的必要性：
* 查询表达往往含糊或歧义，仅靠向量相似度易误召、漏召。
* 法律文书篇幅长、结构复杂，准确理解查询与文书的深层匹配关系，必须依赖更强的语言理解能力。

### 混合嵌入与索引选择

为实现密集检索、稀疏检索和混合检索，我们需要为每个 chunk 生成密集与稀疏两种向量，并为它们在 Milvus 向量数据库中配置适当的索引。

#### 密集向量 (Dense Vector) 与其索引

* 向量生成：使用 qwen3-embedding-0.6b 模型生成密集向量，该模型不支持生成稀疏向量。
* 索引选择：对于存储在内存中的大规模密集向量，选择合适的近似最近邻（ANN）索引是平衡性能、成本和精度的关键。以下是几种业界主流索引的客观比较：

| 索引类型 | 核心原理                                                                                                    | 主要优势与考量                                                                                                 | 典型适用场景                                                                 |
| -------- | ----------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| IVF      | 通过聚类将向量分组，并建立倒排索引（Inverted File）。搜索时先筛选部分分组（候选集），再在其中进行精确搜索。 | 优势：构建速度快，内存占用显著低于图索引。<br/>考量：性能和精度严重依赖 nprobe (查询的候选集数量) 参数的调节。 | 中等规模数据集（百万级），对资源消耗（内存）和索引构建速度有较高要求的场景。 |
| HNSW     | 构建多层图结构（Hierarchical Navigable Small World）实现高效近邻搜索，利用分层导航和贪婪路由加速查找过程。  | 优势：精度高，查询速度快。<br/>考量：构建成本和内存占用相对较高。                                              | 需要低延迟和高精度查询的高维数据集。                                         |
| SCANN    | 结合向量量化（压缩）、聚类分桶和多阶段搜索等技术来加速搜索。                                                | 优势：查询速度和精度优秀，内存占用较低。<br/>考量：对硬件性能有一定要求，支持多种优化策略。                    | 需要高吞吐量的大规模数据集。                                                 |
| DiskANN  | 针对磁盘（SSD）存储优化的图索引算法，将大部分数据和索引结构保存在磁盘上。                                   | 优势：支持海量（十亿级以上）向量索引，内存占用极低。<br/>考量：查询速度受限于磁盘 I/O，但仍远快于暴力搜索。    | 数据规模极大，无法全部加载到内存的场景。                                     |

选择考量：这几种索引覆盖了从资源敏感型到性能优先型的不同需求。结合我们的应用需求——优先保障低延迟和高召回，对吞吐量要求适中，[top-k 较小](https://milvus.io/docs/index-explained.md)，同时需保留未来扩展数据规模的能力——HNSW 在延迟与召回之间实现了良好平衡，是当前主流高性能应用中的优选方案。对于 HNSW 的参数，我们选用一套高召回率的设置（M=24、efConstruction=400），代价是需要更多的内存和构建时间。

#### 稀疏向量 (Sparse Vector) 与倒排索引

* 向量生成：使用 bge-m3 模型生成稀疏向量。
* 索引选择：唯一的选择就是 SPARSE_INVERTED_INDEX，它利用了倒排索引的原理，为稀疏数据创建了一种高效的搜索结构。

通过为不同特性的向量配置专用索引，我们能够最大化数据库的检索性能，为整个系统提供低延迟、高精度的检索能力，从而为上层的混合检索算法提供高质量的输入。

### 父子分段

对于裁判文书这类篇幅长、逻辑结构强的文档，设定几百长度（例如 256、512）的 chunk_size，会导致案情描述、控辩双方观点、法院认定和判决结果等核心内容分散到多个 chunk，检索出的单个分块往往是上下文不完整的片段，不足以完整、全面地回答用户的问题。而设置更长的 chunk_size，又会影响检索的准确性。

因此我们采用父子分段策略：向量检索基于更小粒度的子分块，以提升语义匹配的精准度；但在生成回答时，系统回溯并使用包含更加完整的上下文的父段落，确保大语言模型理解充分、回答准确。

### LLM 提取元数据

案件的裁判文书中包含大量的细节信息，例如日期、地点、人物、数额等，使用它们作为关键词检索时往往效果不佳（对于罕见的人名地名，采用稀疏向量检索效果尚可），可它们又的确是检索的重要参考信息。既然如此，我们可以利用 LLM 从 chunk 中提取这些关键信息作为额外的元数据，供过滤使用。

相较于依赖正则表达式（Rule-based）的脆弱性和手动标注（Manual）的高成本，LLM 凭借其强大的上下文理解和零样本/少样本学习能力，可以更鲁棒、更规模化地从非结构化文本中抽取出如 {"dates": "20191018", "locations": "沈阳市浑南区"} 这样的键值对。

指示 LLM 提取元数据的 prompt 见 [data/insert_data_cail2018.py](../data/insert_data_cail2018.py) 中的 `extract_metadata_with_llm` 函数。

## Agent 设计

### 识别用户意图

在识别用户意图环节，考虑以下 2 种方案：

1. LLM 启用思考：Qwen3 系列模型支持思考模式（thinking mode）和非思考模式（non-thinking mode），并且在思考模式下支持设置 thinking budget。只需启用思考模式，LLM 就会自主在思考过程中识别用户意图。增加 thinking budget，LLM 对于用户意图的思考也会加深。
2. Workflow 使用问题分类节点：在 Dify 等 LLM 应用开发平台的 Workflow 中，使用平台提供的问题分类节点，根据分类结果路由到不同的分支。

比较 2 种方案：

1. 方案 1 允许用户灵活设置思考的深度，而方案 2 的问题分类过程是固定的；
2. 方案 2 需要为问题分类节点编写的 prompt 更加复杂；
3. 方案 2 问题分类有哪些类型需要由人工定义，离散的类型也不够灵活；

因此我们选择方案 1。

### 检索知识库

对于应用如何检索知识库，考虑以下 2 种方案：

1. 自主动态调用 MCP 工具：将多种检索方法（包含稀疏检索、密集检索、混合检索、元数据过滤、重排等）的实现封装成多个独立的 MCP 工具，通过部署 MCP Server 暴露，LLM 通过 MCP Client 调用。
2. Workflow 使用知识检索节点：编写与 Dify 等 LLM 应用开发平台兼容的外部知识服务并部署，其中实现了多种检索方法；然后创建外部知识库连接外部知识服务，在 Workflow 中使用平台提供的知识检索节点。

比较 2 种方案：

1. 方案 1 在编写 MCP Server 时，我们可以灵活地增减工具，为工具配置适当的参数以实现精细控制，然后自主定义和实现完整的检索流程；而方案 2 的 Dify 外部知识服务只能实现一个固定的 API 端点，参数格式受限，无法扩展。例如，其请求体中无法指定使用的检索方法，也不支持添加新的工具或参数。
2. 方案 1 部署的 MCP Server 与平台解耦，具备高度通用性。当前主流的 AI 应用开发平台（如 LangChain、Dify、n8n 等）均已支持 MCP Client，可灵活部署和迁移，亦可自主开发聊天客户端进行集成。而 Dify 的外部知识服务专属于 Dify 平台，一旦迁移到其他平台，则需重新开发，迁移成本高、通用性差。

因此考虑到可扩展性和可迁移性，我们选择方案 1。

### 管理多轮对话

对于应用在多轮对话中如何管理对话历史，以确保对话历史不超过模型的最大上下文窗口，同时尽量保持上下文的连贯性，我们考虑以下 3 种方案：

1. 保留 token 不超限的对话历史：保留从开始到现在的全部对话，丢弃最旧的对话以控制 token 总数刚好不超过模型的最大上下文窗口。
2. 保留最近 k 轮对话：只保留固定的最近 k 轮对话，无论 token 总数是多少，超过 k 轮的对话都会被丢弃。
3. 总结超限/超轮次的历史对话：当对话超出预设的轮次或长度时，调用 LLM 将早期的对话内容进行总结，形成一段摘要，并将此摘要与近期对话合并作为新的上下文。

比较 3 种方案：

1. 方案 1 通过调整 token limit，即可适用短对话场景，又可适用长对话场景；方案 2 适用于短对话场景；方案 3 适用于需要保留一些背景信息或关键信息的长对话场景
2. 方案 1 通过限制上下文长度保证了推理性能（例如多轮对话的 TTFT）
3. 方案 1 能够灵活适配不同的对话场景（例如每轮对话的 token 数量较少时可以保留很多轮对话）
4. 方案 3 调用 LLM 总结引入额外的计算开销和延迟

法律问答场景下，正确全面的回答往往需要完整的上下文、严谨的表述和丰富的细节信息，有损压缩过的总结对于回答的准确性可能有不利影响。方案 3 可能不完全适用于当前场景。再考虑到方案 1 的上述优势，我们这里选择方案 1。

## 总结和展望

我们构建了一个基于 RAG 与 MCP 框架的企业级智能法律问答应用，集成了法律法规和海量案例数据，结合多种检索方式与重排机制，并通过 LLM 提取元数据、采用父子分块策略，显著提升了检索效果和问答准确率。同时，借助 Qwen3-32B 模型驱动的智能 Agent，可动态调用工具、深入理解用户意图，满足企业对私有化部署和数据安全的高要求。

展望未来，该架构具备良好的通用性和可扩展性，有望推广至金融、医疗、科研等多个专业领域。随着数据规模不断扩大，并引入合同、规章等多样化文档，我们也将逐步构建法律知识图谱，以结构化方式表达复杂的法规和案例关联，从而进一步增强系统的理解力和可解释性，尽管这一过程在技术和资源上仍面临不小挑战。
